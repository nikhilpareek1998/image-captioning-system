# -*- coding: utf-8 -*-
"""image_captioning_fine_tune_vit_bert_model_flickr8k.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pEtUt3oBc4M7Qlhbc91LUhbcsE0Sp7Gx
"""

import pandas as pd
import numpy as np
from tqdm import tqdm
from PIL import Image
!pip install -U accelerate
!pip install -U transformers
!pip install transformers[torch]
!pip install accelerate -U
!pip install rouge_score
!pip install evaluate
from transformers import AutoTokenizer
import evaluate
import torch
from PIL import Image
import matplotlib.pyplot as plt

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d adityajn105/flickr8k

!unzip flickr8k.zip

"""# Importing Text Data"""

text_data = pd.read_csv('captions.txt', sep=',')
text_data

"""# Data Preparation"""

text_data.rename(columns={"image":"image_repeat","caption":"caption"},inplace=True)
text_data

from IPython.display import Image, display

# Replace 'path/to/your/image.jpg' with the actual path to your image file
image_path = '/content/Images/1000268201_693b08cb0e.jpg'
# Display the image
display(Image(filename=image_path))

# Drop rows with given value of column
text_data_train = text_data.drop(text_data[text_data['image_repeat']==0].index)
text_data_train.reset_index(drop=True, inplace=True)
# text_data_train = text_data[text_data['image_repeat']==1]
# text_data_train.reset_index(drop=True, inplace=True)
text_data_val = text_data[ text_data['image_repeat']==0 ]
text_data_val.reset_index(drop=True, inplace=True)
text_data_train.shape, text_data_val.shape

"""# Vision Encoder Decoder Models"""

batch_size = 32
epochs = 20

from transformers import AutoImageProcessor, AutoTokenizer, VisionEncoderDecoderModel
from PIL import Image
import requests
image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
decoder_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", add_special_tokens=True)
if decoder_tokenizer.pad_token is None:
    decoder_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained("google/vit-base-patch16-224-in21k", "bert-base-uncased")
model.config.decoder_start_token_id = decoder_tokenizer.cls_token_id
model.config.pad_token_id = decoder_tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size

from torch.utils.data import Dataset
# from datasets import Dataset
image_height = 224
image_width = 224

class ImageCaptionDataset(Dataset):
    def __init__(self, image_path, text_df, tokenizer, image_height,image_width):
        self.image_path = image_path
        self.text_df = text_df
        self.tokenizer = tokenizer
        self.image_height = image_height
        self.image_width = image_width

    def __len__(self):
        return self.text_df.shape[0]

    def __getitem__(self,index):
        path_filename = self.image_path + text_data_train.iloc[index]['image_repeat']
        image = np.array(Image.open( path_filename ).resize((self.image_width, self.image_height)), dtype=np.uint8)
        image = torch.tensor(np.moveaxis(image, -1, 0))
        caption_token = self.tokenizer( text_data_train.iloc[index]['caption'], add_special_tokens=True, padding="max_length", max_length=32, truncation = True  ).input_ids
        caption_token = [token if token != self.tokenizer.pad_token_id else -100 for token in caption_token]
        return {"pixel_values": image.squeeze(), "caption_token": torch.tensor(caption_token)} #, "caption":self.captions[index]}


train_dataset = ImageCaptionDataset( image_path="/content/Images", text_df=text_data_train , tokenizer=decoder_tokenizer, image_height=224, image_width=224 )
val_dataset = ImageCaptionDataset( image_path="/content/Images", text_df=text_data_val , tokenizer=decoder_tokenizer, image_height=224, image_width=224 )

from torch.utils.data import DataLoader
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size)

"""# Training Setup"""

from torch.optim import AdamW
optimizer = AdamW(model.parameters(), lr=5e-5)

from transformers import get_scheduler
num_training_steps = epochs * len(train_dataloader)
lr_scheduler = get_scheduler( name="linear", optimizer=optimizer, num_warmup_steps=int(num_training_steps/5), num_training_steps=num_training_steps )


if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

print(device)
# Set GPU/CPU
model.to(device)

"""# Training Model"""

from tqdm.auto import tqdm
progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(epochs):
    losses = []
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(pixel_values = batch['pixel_values'], labels=batch['caption_token']) # decoder_input_ids=batch['caption_token'],
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
        losses.append(loss)

    print('Epoch: {}, Loss: {:.4f}'.format(epoch, sum(losses)))

"""# Evaluating Model"""

metric = evaluate.load('rouge')

model.eval()
losses = []
for batch in val_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(pixel_values=batch['pixel_values'],labels=batch['caption_token'])

    logits = outputs.logits
    loss = outputs.loss
    losses.append(loss)
    predictions = torch.argmax(logits, dim=-1)
    predictions = decoder_tokenizer.batch_decode(predictions,skip_special_tokens=True)
    predictions = [ pred_sen.replace(".","") for pred_sen in predictions ]
    references = [decoder_tokenizer.decode( [0 if token ==-100 else token for token in ref ] ) for ref in batch['caption_token'].tolist() ]
    metric.add_batch( predictions=predictions, references=references )

val_rouge_score = metric.compute()
print( " Validation Loss: ", torch.sum(torch.tensor(losses)) )
print(" Rouge Score on Validation Set: ", val_rouge_score )

"""# Generating Captions"""

def generate_caption(image, model):

    image = torch.unsqueeze( torch.tensor(image), 0)
    predictions = decoder_tokenizer.decode(model.generate(pixel_values=image.to(device))[0],skip_special_tokens=True)
    caption = predictions.replace(".","")

    image = np.moveaxis(image[0].numpy(),0,-1)
    plt.figure(figsize=(4,4))
    plt.imshow(image)
    plt.axis('off')
    plt.show()
    print( " Generated Caption:  ", caption )
    return caption

pred_caption = generate_caption(image=train_dataset.__getitem__(0)['pixel_values'].numpy(), model=model)



"""***"""